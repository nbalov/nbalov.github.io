<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>Titanic</title>

  <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
  <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

  <!--BOOTSTRAP-->
  <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">
  <link href="../../css/default.css" rel="stylesheet">
  <link href="../../highlight/styles/github.css" rel="stylesheet">

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-98701592-1', 'auto');
  ga('require', 'linkid', 'linkid.js');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');
  </script>

</head>

<body>

<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand active" href="../../index.html" style="font-size:20px;"><span class="glyphicon glyphicon-home"></span> Home</a>
    </div><!-- navbar-header -->

	  <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
        <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
      </ul>
    </div><!-- navbar-collapse -->

  </div><!-- container -->
</nav>

<div id="content">
	<div class="container">
		<div class="row">
			<div class="col-md-10">

<h1>Surviving Titanic Disaster</h1>
        
<div class="info">
	<p style="font-family:CMSS; font-size:100%">Posted on February 08, 2018</p>
</div>

<p><a href="https://www.kaggle.com/c/titanic">Titanic: Machine Learning from Disaster</a> is 
a Kaggle competition about the sinking of the infamous RMS Titanic. 
The challenge is to predict which passengers survived the disaster.</p>

<p>You can download the training data from <a href="https://www.kaggle.com/c/titanic/data">here</a> and import the dataset in Stata as a csv-file.
<pre>. <b>import delimited train.csv</b>
(12 vars, 891 obs)
</pre>
</p>

<p>From the 12 variables in the dataset I decide to use only 7 of them: 
<b>survived</b> (binary outcome variable), <b>sex</b>, <b>age</b>, 
<b>pclass</b> (ticket class), <b>sibsp</b> (number of siblings/spouses),  
<b>parch</b> (number of parents/children), and <b>fare</b> (passenger fare). 
I need all of my variables to be numeric, so I encode the string variable 
<b>sex</b> into a binary variable, <b>bsex</b>.
<pre>. <b>egen bsex = group(sex)</b>
</pre>
Although the training dataset includes a total of 891 passengers, the number of 
complete records is only 741. Hereafter I simply discard the incomplete records.  
However, a more comprehensive approach would try to impute the missing values 
and utilize the available data entirely. For maximum performance, one would 
use all variables or may construct additional specially designed variables in 
order to catch more subtle interactions. My goal however is different. <p>

<p>I want to fit and compare two models for predicting passenger survival: 
a logistic regression model and a multilayer perceptron.  It is a comparison of 
a standard statistical methodology, logistic regression, to a very different 
analytical tool which is outside the mainstream statistics. Each has its pros and cons.</p>

<p>I use about two-thirds (first 600 records) of the dataset for training and 
one-third for validation. Then I compare the prediction performance of the two models.</p>

<h2>A logistic regression model</h2>

<p>Let's first consider a logistic regression model with outcome 
<b>survived</b> and independent predictor variables <b>sex</b>, <b>age</b>, 
<b>pclass</b>, <b>sibsp</b>,  <b>parch</b>, and <b>fare</b>. 
Fitting the model with the <b>logit</b> command is straightforward. </p>

<pre>. <b>logit survived bsex age pclass sibsp parch fare in 1/600, or</b>

Iteration 0:   log likelihood = -321.42313  
Iteration 1:   log likelihood = -221.62832  
Iteration 2:   log likelihood = -220.06392  
Iteration 3:   log likelihood = -220.05786  
Iteration 4:   log likelihood = -220.05786  

Logistic regression                             Number of obs     =        474
                                                LR chi2(6)        =     202.73
                                                Prob > chi2       =     0.0000
Log likelihood = -220.05786                     Pseudo R2         =     0.3154

------------------------------------------------------------------------------
    survived | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        bsex |   .0738699   .0188864   -10.19   0.000     .0447549    .1219256
         age |   .9633228   .0094899    -3.79   0.000     .9449014    .9821034
      pclass |   .3360143   .0675789    -5.42   0.000     .2265505    .4983685
       sibsp |   .6957472   .1035633    -2.44   0.015      .519695    .9314389
       parch |   1.029911   .1696705     0.18   0.858     .7457102    1.422424
        fare |   .9984295   .0032647    -0.48   0.631     .9920513    1.004849
       _cons |    1872.28   1634.887     8.63   0.000     338.1398    10366.82
------------------------------------------------------------------------------
Note: _cons estimates baseline odds.
</pre>

<p>The results are unsurprising.  Age, having a sibling or a spouse, traveling 
a lower class, and being a man all adversely affect your chances of survival. 
On the other hand, having a child or a parent increase your chances. Sex and 
ticket class seem to have the most impact. For example, the odds of survival of 
men vs. women are only 7 to 100. Among the clear advantages of the logistic 
regression model are the interpretability of regression coefficients and 
the rigorous criteria of their statistical significance in terms of Z-test p-values.</p>

<p>The next step is to make predictions based on the just fitted model. 
I assess the prediction performance on the last third of the dataset, which was 
not used for training - these are observations from 601 to 891. 
I use prediction accuracy as a performance criteria.</p>

<pre>
. <b>predict logitpred</b>
. <b>replace logitpred = floor(0.5+logitpred)</b>
. <b>generate match = logitpred == survived</b>
. <b>summarize match in 601/891</b>

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
       match |        291    .6597938    .4745945          0          1
</pre>

<p>The validation accuracy of the logistic regression model is only 
about 66%, which is not great. In fact, if we had predicted all passengers to 
have perished, we would be right in 64% of the cases. So, despite all of the 
insights it provides, the logistic model has little to offer in terms of 
effective prediction.</p>

<h2>A neural network predictive model</h2>

<p>Let's now consider a multilayer perceptron with 2 hidden layers as a simple 
neural network predictive model and see how it compares to the logistic 
regression model. I specify the model using the <b>mlp2</b> command introduced 
<a href="../mlp2/mlp2.html">here</a> and let the 2 hidden layers have 
10 neurons each. The only additional option I supply is <b>lrate(0.5)</b> to 
increase the default learning rate of the optimizer.</p>

<pre>
. <b>set seed 12345</b>
. <b>mlp2 fit survived bsex age pclass sibsp parch fare in 1/600, layer1(10) layer2(10) lrate(0.5)</b>

------------------------------------------------------------------------------
Multilayer perceptron                              input variables =        6
                                                   layer1 neurons  =       10
                                                   layer2 neurons  =       10
Loss: softmax                                      output levels   =        2

Optimizer: sgd                                     batch size      =       50
                                                   max epochs      =      100
                                                   loss tolerance  =    .0001
                                                   learning rate   =       .5

Training ended:                                    epochs          =      100
                                                   start loss      =  .579942
                                                   end loss        =  .355872
------------------------------------------------------------------------------
</pre>
<p>After 100 optimization steps, the default number of steps, the optimization 
loss has decreased from about 0.58 to 0.36. Beside the fact that some learning took 
place, we have little indication of the model fit. My choice of the size of the 
hidden layers is specifically for the purpose of keeping the network small and 
avoiding severe overfitting.</p>

<p>Again, I use the last third of the dataset to validate the prediction 
performance of the model.</p>

<pre>
. <b>mlp2 predict in 601/891, genvar(ypred)</b>

Prediction accuracy: .8666666666666667
</pre>

<p>
We achieve an accuracy of about 87%, which is notably better than the 
performance of the logistic regression model. To be fare, having 202 parameters, 
the neural network is a substantially larger model than the 7-parameter logistic 
regression. 
</p>

<p>
On the downside, the neural network tells us very little in terms of predictor 
importance and statistical significance. Except empirical evidence, there are no 
guarantees that the model is systematically better than logistic regression. 
That rules out its use in applications where statistical rigor is required.  
</p>

      </div><!--/col-md-10-collapse -->
    </div><!--/row-collapse -->
  </div><!--/container-collapse -->
</div><!--/content-collapse -->

<script src="../../js/jquery-3.3.1.min.js"></script>
<script src="../../js/inlineDisqussions.js"></script>
<script src="../../highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="../../js/footnotes.js"></script>

<div id="footer">
  <div class="container">
    using <a href="http://getbootstrap.com/">Bootstrap</a>,
    <a href="http://www.mathjax.org/">MathJax</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>,
    and <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>.
  </div>
</div>

<noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

</body>



